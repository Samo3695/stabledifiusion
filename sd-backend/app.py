from flask import Flask, request, jsonify
from flask_cors import CORS
import torch
from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler
from PIL import Image
import io
import base64
import os

app = Flask(__name__)
CORS(app)  # Povol√≠ po≈æiadavky z Vue aplik√°cie

# Glob√°lna premenn√° pre pipeline
pipe = None

def load_model():
    """Naƒç√≠ta Stable Diffusion model"""
    global pipe
    
    print("üîÑ Naƒç√≠tavam Stable Diffusion model...")
    print("‚è≥ Prv√© spustenie m√¥≈æe trva≈• niekoƒæko min√∫t (s≈•ahuje sa ~4GB model)")
    
    # Zistenie dostupnosti GPU
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"üñ•Ô∏è  Pou≈æ√≠vam zariadenie: {device}")
    
    if device == "cpu":
        print("‚ö†Ô∏è  UPOZORNENIE: Generovanie na CPU je veƒæmi pomal√© (1-5 min√∫t)")
    
    # Naƒç√≠tanie modelu z Hugging Face
    model_id = "runwayml/stable-diffusion-v1-5"
    
    try:
        pipe = StableDiffusionPipeline.from_pretrained(
            model_id,
            torch_dtype=torch.float16 if device == "cuda" else torch.float32,
            safety_checker=None,  # Vypnutie safety checkera pre r√Ωchlos≈•
        )
        
        # Optimaliz√°cia
        pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)
        pipe = pipe.to(device)
        
        # Pre GPU - optimaliz√°cia pam√§te
        if device == "cuda":
            pipe.enable_attention_slicing()
            # pipe.enable_xformers_memory_efficient_attention()  # Vy≈æaduje xformers
        
        print("‚úÖ Model √∫spe≈°ne naƒç√≠tan√Ω!")
        return True
        
    except Exception as e:
        print(f"‚ùå Chyba pri naƒç√≠tavan√≠ modelu: {e}")
        return False

@app.route('/health', methods=['GET'])
def health():
    """Kontrola stavu servera"""
    return jsonify({
        'status': 'ok',
        'model_loaded': pipe is not None,
        'device': 'cuda' if torch.cuda.is_available() else 'cpu'
    })

@app.route('/generate', methods=['POST'])
def generate():
    """Generuje obr√°zok z promptu"""
    global pipe
    
    if pipe is None:
        return jsonify({'error': 'Model nie je naƒç√≠tan√Ω'}), 500
    
    try:
        data = request.json
        prompt = data.get('prompt', '')
        negative_prompt = data.get('negative_prompt', '')
        num_inference_steps = data.get('num_inference_steps', 50)
        guidance_scale = data.get('guidance_scale', 7.5)
        
        if not prompt:
            return jsonify({'error': 'Prompt je povinn√Ω'}), 400
        
        print(f"üé® Generujem: {prompt}")
        
        # Generovanie obr√°zka
        with torch.inference_mode():
            image = pipe(
                prompt=prompt,
                negative_prompt=negative_prompt,
                num_inference_steps=num_inference_steps,
                guidance_scale=guidance_scale,
                width=512,
                height=512,
            ).images[0]
        
        # Konverzia na base64
        buffer = io.BytesIO()
        image.save(buffer, format='PNG')
        img_base64 = base64.b64encode(buffer.getvalue()).decode()
        
        print("‚úÖ Obr√°zok vygenerovan√Ω!")
        
        return jsonify({
            'image': f'data:image/png;base64,{img_base64}',
            'prompt': prompt
        })
        
    except Exception as e:
        print(f"‚ùå Chyba pri generovan√≠: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/models', methods=['GET'])
def list_models():
    """Zoznam dostupn√Ωch modelov"""
    models = [
        {
            'id': 'runwayml/stable-diffusion-v1-5',
            'name': 'Stable Diffusion 1.5',
            'description': '≈†tandardn√Ω model, dobr√° rovnov√°ha kvality a r√Ωchlosti'
        },
        {
            'id': 'stabilityai/stable-diffusion-2-1',
            'name': 'Stable Diffusion 2.1',
            'description': 'Nov≈°ia verzia, lep≈°ia kvalita'
        }
    ]
    return jsonify({'models': models})

if __name__ == '__main__':
    print("=" * 60)
    print("üöÄ Stable Diffusion Backend Server")
    print("=" * 60)
    
    # Naƒç√≠tanie modelu pri ≈°tarte
    if load_model():
        print("\nüåê Server je pripraven√Ω!")
        print("üìç URL: http://localhost:5000")
        print("=" * 60)
        app.run(host='0.0.0.0', port=5000, debug=False)
    else:
        print("\n‚ùå Nepodarilo sa naƒç√≠ta≈• model. Server sa nespust√≠.")
